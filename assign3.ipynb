{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils import * # data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 20000\n",
      "y_train: 20000\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False) # if final is True, train and test set is returned. Else validation data\n",
    "\n",
    "print('x_train:', len(x_train))\n",
    "print('y_train:', len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return values are as follows:\n",
    "\n",
    "● x_train A python list of lists of integers. Each integer represents a word. Sorted\n",
    "from short to long.\n",
    "\n",
    "● y_train The corresponding class labels: 0 for positive, 1 for negative.\n",
    "\n",
    "● x_val Test/validation data. Laid out the same as x_train.\n",
    "\n",
    "● y_val Test/validation labels\n",
    "\n",
    "● i2w A list of strings mapping the integers in the sequences to their original words.\n",
    "i2w[141] returns the string containing word 141.\n",
    "\n",
    "● w2i A dictionary mapping the words to their indices. w2i['film'] returns the index\n",
    "for the word \"film\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0]) # each integer represents a word, shorted from short to long\n",
    "print(y_train[0]) # 0 or 1, 0 means negative, 1 means positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i2w) # index to word (list)\n",
    "print(w2i) # word to index (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2i['pad']) # uses to fill the sentence to the same length with\n",
    "print(w2i['start'])\n",
    "print(w2i['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in x_train[:10]:\n",
    "    for word_index in sentence:\n",
    "        print(i2w[word_index], end=' ')\n",
    "    print(len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(x, y, w2i, batch_size = 16):\n",
    "    \n",
    "    batches_x = []\n",
    "    batches_y = []\n",
    "    \n",
    "    # step over x met steps of batch_size\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        \n",
    "        start = i\n",
    "        end = i + batch_size\n",
    "        \n",
    "        # get the batch\n",
    "        batch_x = x[start:end]\n",
    "        batch_y = y[start:end]\n",
    "        \n",
    "        \n",
    "        \n",
    "        batch = []\n",
    "        for i, sentence in enumerate(batch_x):\n",
    "            longest_sentence = max([len(sentence) for sentence in batch_x])\n",
    "            if len(sentence) < longest_sentence:\n",
    "                sentence += [w2i['pad']] * (longest_sentence - len(sentence))\n",
    "\n",
    "            # print(len(sentence))\n",
    "            batch.append(sentence)\n",
    "        \n",
    "        batches_x.append(batch)\n",
    "        batches_y.append(batch_y)\n",
    "        \n",
    "    # transform all batches to tensors\n",
    "    batches_x = [torch.tensor(batch, dtype = torch.long) for batch in batches_x]\n",
    "    batches_y = [torch.tensor(batch, dtype = torch.long) for batch in batches_y]\n",
    "        \n",
    "    return batches_x, batches_y\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batches\n",
    "batch_size = 16\n",
    "batches_x, batches_y = padding(x_train, y_train, w2i, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, w2i, embedding_dim = 300, hidden_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        num_embeddings = len(w2i)\n",
    "        self.embedding =  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.hidden = torch.nn.Linear(embedding_dim, hidden_size)\n",
    "        self.output = torch.nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        k = self.hidden(emb)\n",
    "        h = torch.nn.functional.relu(k)\n",
    "        o, _ = torch.max(h, dim=1)\n",
    "        y = self.output(o)\n",
    "        return y \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batches_x, batches_y, model, epochs = 5, optimizer = 'Adam', lr=0.001):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    batches_x = [batch.to(device) for batch in batches_x]\n",
    "    batches_y = [batch.to(device) for batch in batches_y]\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for i, batch in enumerate(batches_x):\n",
    "            if i % 100 == 0: print(i)\n",
    "            predicted_y = model(batch)\n",
    "            loss = torch.nn.functional.cross_entropy(predicted_y, batches_y[i])\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # get index of the max value (0 or 1)\n",
    "            predicted_y = predicted_y.argmax(dim=1)\n",
    "            \n",
    "            # calculate accuracy: number of correct predictions / number of predictions\n",
    "            n_correct = (predicted_y == batches_y[i]).sum().item()\n",
    "            accuracy = n_correct / len(predicted_y)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "        print('Epoch: ', epoch, 'Loss: ', np.mean(losses), 'Accuracy: ', np.mean(accuracies))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(w2i)\n",
    "train(batches_x, batches_y, model, epochs = 10, optimizer = 'Adam', lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(torch.nn.Module):\n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(insize + hsize, hsize)  # Input-to-hidden layer\n",
    "        self.lin2 = torch.nn.Linear(hsize, outsize)  # Hidden-to-output layer\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size, sequence_size, embedding_size = x.size()\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size, embedding_size, dtype=torch.float)\n",
    "        \n",
    "        outs = []\n",
    "        for i in range(sequence_size): \n",
    "            inp = torch.cat([x[:, i, :], hidden], dim=1)\n",
    "            hidden = torch.nn.functional.relu(self.lin1(inp))\n",
    "            out = self.lin2(hidden)\n",
    "            outs.append(out[:, None, :])\n",
    "\n",
    "        return torch.cat(outs, dim=1), hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, w2i, embedding_dim = 300, hidden_size = 300):\n",
    "        super(MLP2, self).__init__()\n",
    "        num_embeddings = len(w2i)\n",
    "        self.embedding =  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.hidden = Elman(embedding_dim, hidden_size)\n",
    "        self.output = torch.nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        tensors, hidden_layer = self.hidden(emb)\n",
    "        h = torch.nn.functional.relu(tensors)\n",
    "        o, _ = torch.max(h, dim=1)\n",
    "        y = self.output(o)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP2(w2i)\n",
    "train(batches_x[:2], batches_y[:2], model, epochs = 10, optimizer = 'Adam', lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementaion of the Elman network with pytorch modules\n",
    "class Elman_torch(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, w2i, embedding_dim = 300, hidden_size = 300):\n",
    "        super(Elman_torch, self).__init__()\n",
    "        num_embeddings = len(w2i)\n",
    "        self.embedding =  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.hidden1 = torch.nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        # self.hidden2 = torch.nn.RNN(embedding_dim, hidden_size)\n",
    "        self.output = torch.nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        tensors, hidden = self.hidden1(emb)\n",
    "        # tensors, hidden = self.hidden2(tensors, hidden)\n",
    "        o, _ = torch.max(tensors, dim=1)\n",
    "        y = self.output(o)\n",
    "        return y \n",
    "    \n",
    "# NOTE: torch.nn.RNN computes only one layer, which is already activated. So no need to use activation function in the forward method. \n",
    "# There are 2 hidden layer since our implementation of the Elman network has 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementaion of the LSTM network with pytorch modules\n",
    "class LSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, w2i, embedding_dim = 300, hidden_size = 300):\n",
    "        super(LSTM, self).__init__()\n",
    "        num_embeddings = len(w2i)\n",
    "        self.embedding =  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.hidden = torch.nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.output = torch.nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        tensors, hidden_layer = self.hidden(emb)\n",
    "        h = torch.nn.functional.relu(tensors)\n",
    "        o, _ = torch.max(h, dim=1)\n",
    "        y = self.output(o)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elman model\n",
    "model = Elman_torch(w2i)\n",
    "train(batches_x[:2], batches_y[:2], model, epochs = 2, optimizer = 'Adam', lr=0.001)\n",
    "\n",
    "# lstm model\n",
    "model = LSTM(w2i)\n",
    "train(batches_x[:2], batches_y[:2], model, epochs = 2, optimizer = 'Adam', lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
