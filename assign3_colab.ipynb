{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDuFNRB20D79",
        "outputId": "0d59c669-e230-48df-98af-030528f49576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec  2 14:10:46 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i_Okz6RzAKQ",
        "outputId": "d9fe2219-d776-42ee-a0c9-30ab137dc636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=cef4261c71a749ec8c8cd7d596b36b99d35c32d3fc6ec6ee19d54f7f1718f8da\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement gzip (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gzip\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sys (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sys\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting ray[tune]\n",
            "  Downloading ray-2.8.1-cp310-cp310-manylinux2014_x86_64.whl (62.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (23.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.5.3)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (9.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n",
            "Installing collected packages: tensorboardX, ray\n",
            "Successfully installed ray-2.8.1 tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install os\n",
        "!pip install gzip\n",
        "!pip install pickle\n",
        "!pip install random\n",
        "!pip install re\n",
        "!pip install sys\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install ray[tune]\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import wget\n",
        "import os\n",
        "import gzip\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "from ray import train as ray_train\n",
        "from ray import tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vg67Q81XzZVP"
      },
      "outputs": [],
      "source": [
        "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
        "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
        "\n",
        "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
        "\n",
        "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
        "\n",
        "    cst = 'char' if char else 'word'\n",
        "\n",
        "    imdb_url = IMDB_URL.format(cst)\n",
        "    imdb_file = IMDB_FILE.format(cst)\n",
        "\n",
        "    if not os.path.exists(imdb_file):\n",
        "        wget.download(imdb_url)\n",
        "\n",
        "    with gzip.open(imdb_file) as file:\n",
        "        sequences, labels, i2w, w2i = pickle.load(file)\n",
        "\n",
        "    if voc is not None and voc < len(i2w):\n",
        "        nw_sequences = {}\n",
        "\n",
        "        i2w = i2w[:voc]\n",
        "        w2i = {w: i for i, w in enumerate(i2w)}\n",
        "\n",
        "        mx, unk = voc, w2i['.unk']\n",
        "        for key, seqs in sequences.items():\n",
        "            nw_sequences[key] = []\n",
        "            for seq in seqs:\n",
        "                seq = [s if s < mx else unk for s in seq]\n",
        "                nw_sequences[key].append(seq)\n",
        "\n",
        "        sequences = nw_sequences\n",
        "\n",
        "    if final:\n",
        "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
        "\n",
        "    # Make a validation split\n",
        "    random.seed(seed)\n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    x_val, y_val = [], []\n",
        "\n",
        "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
        "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
        "        if i in val_ind:\n",
        "            x_val.append(s)\n",
        "            y_val.append(l)\n",
        "        else:\n",
        "            x_train.append(s)\n",
        "            y_train.append(l)\n",
        "\n",
        "    return (x_train, y_train), \\\n",
        "           (x_val, y_val), \\\n",
        "           (i2w, w2i), 2\n",
        "\n",
        "\n",
        "def gen_sentence(sent, g):\n",
        "\n",
        "    symb = '_[a-z]*'\n",
        "\n",
        "    while True:\n",
        "\n",
        "        match = re.search(symb, sent)\n",
        "        if match is None:\n",
        "            return sent\n",
        "\n",
        "        s = match.span()\n",
        "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
        "\n",
        "def gen_dyck(p):\n",
        "    open = 1\n",
        "    sent = '('\n",
        "    while open > 0:\n",
        "        if random.random() < p:\n",
        "            sent += '('\n",
        "            open += 1\n",
        "        else:\n",
        "            sent += ')'\n",
        "            open -= 1\n",
        "\n",
        "    return sent\n",
        "\n",
        "def gen_ndfa(p):\n",
        "\n",
        "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
        "\n",
        "    s = ''\n",
        "    while True:\n",
        "        if random.random() < p:\n",
        "            return 's' + s + 's'\n",
        "        else:\n",
        "            s+= word\n",
        "\n",
        "def load_brackets(n=50_000, seed=0):\n",
        "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
        "\n",
        "def load_ndfa(n=50_000, seed=0):\n",
        "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
        "\n",
        "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    if name == 'lang':\n",
        "        sent = '_s'\n",
        "\n",
        "        toy = {\n",
        "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
        "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
        "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
        "            '_prep': ['on', 'with', 'to'],\n",
        "            '_con' : ['while', 'but'],\n",
        "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
        "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
        "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
        "        }\n",
        "\n",
        "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
        "        sentences.sort(key=lambda s : len(s))\n",
        "\n",
        "    elif name == 'dyck':\n",
        "\n",
        "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
        "        sentences.sort(key=lambda s: len(s))\n",
        "\n",
        "    elif name == 'ndfa':\n",
        "\n",
        "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
        "        sentences.sort(key=lambda s: len(s))\n",
        "\n",
        "    else:\n",
        "        raise Exception(name)\n",
        "\n",
        "    tokens = set()\n",
        "    for s in sentences:\n",
        "\n",
        "        if char:\n",
        "            for c in s:\n",
        "                tokens.add(c)\n",
        "        else:\n",
        "            for w in s.split():\n",
        "                tokens.add(w)\n",
        "\n",
        "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
        "    t2i = {t:i for i, t in enumerate(i2t)}\n",
        "\n",
        "    sequences = []\n",
        "    for s in sentences:\n",
        "        if char:\n",
        "            tok = list(s)\n",
        "        else:\n",
        "            tok = s.split()\n",
        "        sequences.append([t2i[t] for t in tok])\n",
        "\n",
        "    return sequences, (i2t, t2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukVNsGzvzAKV",
        "outputId": "ef9a705e-0931-4ff9-e296-d73516ed3c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: 20000\n",
            "y_train: 20000\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False) # if final is True, train and test set is returned. Else validation data\n",
        "\n",
        "print('x_train:', len(x_train))\n",
        "print('y_train:', len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xbph7n_LzAKd"
      },
      "outputs": [],
      "source": [
        "def padding(x, y, w2i, batch_size = 16):\n",
        "\n",
        "    batches_x = []\n",
        "    batches_y = []\n",
        "\n",
        "    # step over x met steps of batch_size\n",
        "    for i in range(0, len(x), batch_size):\n",
        "\n",
        "        start = i\n",
        "        end = i + batch_size\n",
        "\n",
        "        # get the batch\n",
        "        batch_x = x[start:end]\n",
        "        batch_y = y[start:end]\n",
        "\n",
        "        batch = []\n",
        "        for i, sentence in enumerate(batch_x):\n",
        "            longest_sentence = max([len(sentence) for sentence in batch_x])\n",
        "            if len(sentence) < longest_sentence:\n",
        "                sentence += [w2i['pad']] * (longest_sentence - len(sentence))\n",
        "\n",
        "            # print(len(sentence))\n",
        "            batch.append(sentence)\n",
        "\n",
        "        batches_x.append(batch)\n",
        "        batches_y.append(batch_y)\n",
        "\n",
        "    # transform all batches to tensors\n",
        "    batches_x = [torch.tensor(batch, dtype = torch.long) for batch in batches_x]\n",
        "    batches_y = [torch.tensor(batch, dtype = torch.long) for batch in batches_y]\n",
        "\n",
        "    return batches_x, batches_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UiTlP4PezAKe"
      },
      "outputs": [],
      "source": [
        "# create batches\n",
        "batch_size = 16\n",
        "batches_x, batches_y = padding(x_train, y_train, w2i, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jI_GaCjjzAKf"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, w2i, embedding_dim = 300, hidden_size = 300):\n",
        "        super(MLP, self).__init__()\n",
        "        num_embeddings = len(w2i)\n",
        "        self.embedding =  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.hidden = torch.nn.Linear(embedding_dim, hidden_size)\n",
        "        self.output = torch.nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        k = self.hidden(emb)\n",
        "        h = torch.nn.functional.relu(k)\n",
        "        o, _ = torch.max(h, dim=1)\n",
        "        y = self.output(o)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implementaion of the Elman network with pytorch modules\n",
        "class Elman_torch(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, w2i, embedding_dim = 300, hidden_size = 300):\n",
        "        super(Elman_torch, self).__init__()\n",
        "        num_embeddings = len(w2i)\n",
        "        self.embedding =  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.hidden1 = torch.nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.output = torch.nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        tensors, hidden = self.hidden1(emb)\n",
        "        o, _ = torch.max(tensors, dim=1)\n",
        "        y = self.output(o)\n",
        "        return y"
      ],
      "metadata": {
        "id": "UfA54kcERplR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementaion of the LSTM network with pytorch modules\n",
        "class LSTM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, w2i, embedding_dim = 300, hidden_size = 300):\n",
        "        super(LSTM, self).__init__()\n",
        "        num_embeddings = len(w2i)\n",
        "        self.embedding =  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.hidden = torch.nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.output = torch.nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        tensors, hidden_layer = self.hidden(emb)\n",
        "        h = torch.nn.functional.relu(tensors)\n",
        "        o, _ = torch.max(h, dim=1)\n",
        "        y = self.output(o)\n",
        "        return y"
      ],
      "metadata": {
        "id": "DSgWXM4YR8xP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "48Vq0QLHzAKg"
      },
      "outputs": [],
      "source": [
        "def train(batches_x, batches_y, model, epochs=10, optimizer='Adam', lr=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    model = model.to(device)\n",
        "    batches_x = [batch.to(device) for batch in batches_x]\n",
        "    batches_y = [batch.to(device) for batch in batches_y]\n",
        "\n",
        "    # Choose the optimizer\n",
        "    if optimizer == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optimizer == 'SGD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "\n",
        "        for i, batch in enumerate(batches_x):\n",
        "            optimizer.zero_grad()  # Reset gradients for each batch\n",
        "            predicted_y = model(batch)\n",
        "            loss = torch.nn.functional.cross_entropy(predicted_y, batches_y[i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted_y = predicted_y.argmax(dim=1)\n",
        "            n_correct = (predicted_y == batches_y[i]).sum().item()\n",
        "            accuracy = n_correct / len(predicted_y)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "        epoch_accuracy = np.mean(accuracies)\n",
        "        print('Epoch:', epoch, 'Loss:', np.mean(losses), 'Accuracy:', epoch_accuracy)\n",
        "        ray_train.report({'accuracy': np.mean(accuracies)})  # Report current accuracy to Ray Tune\n",
        "\n",
        "        # Early stopping logic\n",
        "        if epoch_accuracy > best_accuracy:\n",
        "            best_accuracy = epoch_accuracy\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= 3:\n",
        "            print(\"Stopping early due to no improvement in accuracy.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_to_tune(config, model_class, x, y, w2i):\n",
        "    # Padding is done inside the training loop due to varying batch sizes\n",
        "    batches_x, batches_y = padding(x, y, w2i, batch_size=int(config['batch_size']))\n",
        "\n",
        "    # Initialize and train the model\n",
        "    model = model_class(w2i)\n",
        "    train(batches_x, batches_y, model, optimizer=config['optimizer'], lr=config['lr'], epochs=20)\n",
        "\n",
        "# Define the search space\n",
        "search_space = {\n",
        "    'lr': tune.loguniform(1e-4, 1e-2),\n",
        "    'optimizer': tune.choice(['Adam', 'SGD']),\n",
        "    'batch_size': tune.choice([16, 32, 64]),\n",
        "}\n",
        "\n",
        "model_classes = [MLP, Elman_torch, LSTM]\n",
        "\n",
        "# Run the tuner\n",
        "for model_class in model_classes:\n",
        "    analysis = tune.run(\n",
        "        lambda config: train_to_tune(config, model_class, x_train, y_train, w2i),\n",
        "        config=search_space,\n",
        "        num_samples=20, # number of trials with different hyperparameter values\n",
        "        resources_per_trial={\"cpu\": 0, \"gpu\": 1}, # set everything on cuda\n",
        "    )\n",
        "\n",
        "    print(\"Best config for model\", model_class.__name__, \": \", analysis.get_best_config(metric=\"accuracy\", mode=\"max\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnURO_Qxn5ry",
        "outputId": "8a4705df-13de-4e72-b005-e702f4fbd4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-02 14:12:11,104\tINFO worker.py:1673 -- Started a local Ray instance.\n",
            "2023-12-02 14:12:12,252\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
            "2023-12-02 14:12:12,255\tINFO tune.py:595 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------+\n",
            "| Configuration for experiment     lambda_2023-12-02_14-12-12   |\n",
            "+---------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator        |\n",
            "| Scheduler                        FIFOScheduler                |\n",
            "| Number of trials                 20                           |\n",
            "+---------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/lambda_2023-12-02_14-12-12\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/lambda_2023-12-02_14-12-12`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-02 14:12:24,495\tWARNING worker.py:2074 -- Warning: The actor ImplicitFunc is very large (24 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 16 PENDING\n",
            "Current time: 2023-12-02 14:12:24. Total running time: 12s\n",
            "Logical resource usage: 0/8 CPUs, 1.0/1 GPUs\n",
            "+--------------------------------------------------------------------------+\n",
            "| Trial name           status              lr   optimizer       batch_size |\n",
            "+--------------------------------------------------------------------------+\n",
            "| lambda_c9b7c_00000   PENDING    0.000952192   Adam                    32 |\n",
            "| lambda_c9b7c_00001   PENDING    0.00431589    SGD                     16 |\n",
            "| lambda_c9b7c_00002   PENDING    0.000103798   SGD                     32 |\n",
            "| lambda_c9b7c_00003   PENDING    0.00147511    SGD                     16 |\n",
            "| lambda_c9b7c_00004   PENDING    0.000456514   SGD                     16 |\n",
            "| lambda_c9b7c_00005   PENDING    0.000154257   SGD                     16 |\n",
            "| lambda_c9b7c_00006   PENDING    0.000109883   Adam                    32 |\n",
            "| lambda_c9b7c_00007   PENDING    0.000436546   Adam                    32 |\n",
            "| lambda_c9b7c_00008   PENDING    0.00197639    SGD                     32 |\n",
            "| lambda_c9b7c_00009   PENDING    0.000243215   SGD                     32 |\n",
            "| lambda_c9b7c_00010   PENDING    0.00417097    Adam                    16 |\n",
            "| lambda_c9b7c_00011   PENDING    0.00328783    SGD                     64 |\n",
            "| lambda_c9b7c_00012   PENDING    0.000519536   Adam                    64 |\n",
            "| lambda_c9b7c_00013   PENDING    0.000939795   Adam                    64 |\n",
            "| lambda_c9b7c_00014   PENDING    0.000158191   SGD                     16 |\n",
            "| lambda_c9b7c_00015   PENDING    0.00358828    Adam                    64 |\n",
            "+--------------------------------------------------------------------------+\n",
            "\n",
            "Trial lambda_c9b7c_00000 started with configuration:\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 config             |\n",
            "+---------------------------------------------+\n",
            "| batch_size                               32 |\n",
            "| lr                                  0.00095 |\n",
            "| optimizer                              Adam |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Using device: cuda\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 1 at 2023-12-02 14:12:45. Total running time: 33s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                    17.7017 |\n",
            "| time_total_s                        17.7017 |\n",
            "| training_iteration                        1 |\n",
            "| accuracy                             0.8433 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 0 Loss: 0.3444129933983088 Accuracy: 0.8433\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 2 at 2023-12-02 14:12:53. Total running time: 40s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                    7.43482 |\n",
            "| time_total_s                        25.1365 |\n",
            "| training_iteration                        2 |\n",
            "| accuracy                             0.9123 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 1 Loss: 0.21930918792039156 Accuracy: 0.9123\n",
            "\n",
            "Trial status: 1 RUNNING | 16 PENDING\n",
            "Current time: 2023-12-02 14:12:54. Total running time: 42s\n",
            "Logical resource usage: 0/8 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name           status              lr   optimizer       batch_size     iter     total time (s)     accuracy |\n",
            "+-------------------------------------------------------------------------------------------------------------------+\n",
            "| lambda_c9b7c_00000   RUNNING    0.000952192   Adam                    32        2            25.1365       0.9123 |\n",
            "| lambda_c9b7c_00001   PENDING    0.00431589    SGD                     16                                          |\n",
            "| lambda_c9b7c_00002   PENDING    0.000103798   SGD                     32                                          |\n",
            "| lambda_c9b7c_00003   PENDING    0.00147511    SGD                     16                                          |\n",
            "| lambda_c9b7c_00004   PENDING    0.000456514   SGD                     16                                          |\n",
            "| lambda_c9b7c_00005   PENDING    0.000154257   SGD                     16                                          |\n",
            "| lambda_c9b7c_00006   PENDING    0.000109883   Adam                    32                                          |\n",
            "| lambda_c9b7c_00007   PENDING    0.000436546   Adam                    32                                          |\n",
            "| lambda_c9b7c_00008   PENDING    0.00197639    SGD                     32                                          |\n",
            "| lambda_c9b7c_00009   PENDING    0.000243215   SGD                     32                                          |\n",
            "| lambda_c9b7c_00010   PENDING    0.00417097    Adam                    16                                          |\n",
            "| lambda_c9b7c_00011   PENDING    0.00328783    SGD                     64                                          |\n",
            "| lambda_c9b7c_00012   PENDING    0.000519536   Adam                    64                                          |\n",
            "| lambda_c9b7c_00013   PENDING    0.000939795   Adam                    64                                          |\n",
            "| lambda_c9b7c_00014   PENDING    0.000158191   SGD                     16                                          |\n",
            "| lambda_c9b7c_00015   PENDING    0.00358828    Adam                    64                                          |\n",
            "| lambda_c9b7c_00016   PENDING    0.000813246   SGD                     64                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 3 at 2023-12-02 14:13:00. Total running time: 48s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                     7.4307 |\n",
            "| time_total_s                        32.5672 |\n",
            "| training_iteration                        3 |\n",
            "| accuracy                            0.94585 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 2 Loss: 0.14962758090049028 Accuracy: 0.94585\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 4 at 2023-12-02 14:13:08. Total running time: 55s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                    7.45708 |\n",
            "| time_total_s                        40.0243 |\n",
            "| training_iteration                        4 |\n",
            "| accuracy                            0.96025 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 3 Loss: 0.11145560564175248 Accuracy: 0.96025\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 5 at 2023-12-02 14:13:15. Total running time: 1min 3s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                    7.44107 |\n",
            "| time_total_s                        47.4653 |\n",
            "| training_iteration                        5 |\n",
            "| accuracy                            0.96045 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 4 Loss: 0.11104530232530087 Accuracy: 0.96045\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 6 at 2023-12-02 14:13:23. Total running time: 1min 10s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                    7.46715 |\n",
            "| time_total_s                        54.9325 |\n",
            "| training_iteration                        6 |\n",
            "| accuracy                             0.9824 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 5 Loss: 0.050105048967688345 Accuracy: 0.9824\n",
            "\n",
            "Trial status: 1 RUNNING | 16 PENDING\n",
            "Current time: 2023-12-02 14:13:24. Total running time: 1min 12s\n",
            "Logical resource usage: 0/8 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name           status              lr   optimizer       batch_size     iter     total time (s)     accuracy |\n",
            "+-------------------------------------------------------------------------------------------------------------------+\n",
            "| lambda_c9b7c_00000   RUNNING    0.000952192   Adam                    32        6            54.9325       0.9824 |\n",
            "| lambda_c9b7c_00001   PENDING    0.00431589    SGD                     16                                          |\n",
            "| lambda_c9b7c_00002   PENDING    0.000103798   SGD                     32                                          |\n",
            "| lambda_c9b7c_00003   PENDING    0.00147511    SGD                     16                                          |\n",
            "| lambda_c9b7c_00004   PENDING    0.000456514   SGD                     16                                          |\n",
            "| lambda_c9b7c_00005   PENDING    0.000154257   SGD                     16                                          |\n",
            "| lambda_c9b7c_00006   PENDING    0.000109883   Adam                    32                                          |\n",
            "| lambda_c9b7c_00007   PENDING    0.000436546   Adam                    32                                          |\n",
            "| lambda_c9b7c_00008   PENDING    0.00197639    SGD                     32                                          |\n",
            "| lambda_c9b7c_00009   PENDING    0.000243215   SGD                     32                                          |\n",
            "| lambda_c9b7c_00010   PENDING    0.00417097    Adam                    16                                          |\n",
            "| lambda_c9b7c_00011   PENDING    0.00328783    SGD                     64                                          |\n",
            "| lambda_c9b7c_00012   PENDING    0.000519536   Adam                    64                                          |\n",
            "| lambda_c9b7c_00013   PENDING    0.000939795   Adam                    64                                          |\n",
            "| lambda_c9b7c_00014   PENDING    0.000158191   SGD                     16                                          |\n",
            "| lambda_c9b7c_00015   PENDING    0.00358828    Adam                    64                                          |\n",
            "| lambda_c9b7c_00016   PENDING    0.000813246   SGD                     64                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 7 at 2023-12-02 14:13:30. Total running time: 1min 17s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                    7.46021 |\n",
            "| time_total_s                        62.3927 |\n",
            "| training_iteration                        7 |\n",
            "| accuracy                             0.9927 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 6 Loss: 0.024256554373633117 Accuracy: 0.9927\n",
            "\n",
            "Trial lambda_c9b7c_00000 finished iteration 8 at 2023-12-02 14:13:38. Total running time: 1min 25s\n",
            "+---------------------------------------------+\n",
            "| Trial lambda_c9b7c_00000 result             |\n",
            "+---------------------------------------------+\n",
            "| checkpoint_dir_name                         |\n",
            "| time_this_iter_s                    7.45922 |\n",
            "| time_total_s                        69.8519 |\n",
            "| training_iteration                        8 |\n",
            "| accuracy                             0.9972 |\n",
            "+---------------------------------------------+\n",
            "\u001b[36m(<lambda> pid=1422)\u001b[0m Epoch: 7 Loss: 0.010576792854999075 Accuracy: 0.9972\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}